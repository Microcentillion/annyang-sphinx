<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>PocketSphinx.js</title>
  </head>
  <body>
    <h2>PocketSphinx.js live demo</h2>
    <ul>
      <li>This demo works on recent versions of Chrome and Firefox with the Web Audio API, make sure it works for you and actually records audio.</li>
      <li>Press "Start" and speak</li>
    </ul>
    <select id="grammars"></select>
    <button id="startBtn">Start</button>
    <button id="stopBtn">Stop</button>
    <span id="recording-indicator" style="border-radius: 10px; -moz-border-radius: 10px; -webkit-border-radius: 10px; width: 20px; height: 20px; background: red;"></span>
    <h2>Recognition Output</h2>
    <div id="output" style="height:150px;overflow:auto;" >
    </div>
    <h2>Status</h2>
    <div id="current-status">Loading page</div>

    <script>
      // These will be initialized later
      var recognizer, recorder, callbackManager, audioContext, vizualizer;
      // Only when both recorder and recognizer do we have a ready application
      var recorderReady = recognizerReady = false;

      // A convenience function to post a message to the recognizer and associate
      // a callback to its response
      function postRecognizerJob(message, callback) {
        console.log("[ postRecognizerJob Called ]");
        var msg = message || {};
        if (callbackManager) msg.callbackId = callbackManager.add(callback);
        if (recognizer) recognizer.postMessage(msg);
      };

      // This function initializes an instance of the recorder
      // it posts a message right away and calls onReady when it
      // is ready so that onmessage can be properly set
      function spawnWorker(workerURL, onReady) {
          console.log("[ spawnWorker Called ]");
          recognizer = new Worker(workerURL);
          recognizer.onmessage = function(event) {
            onReady(recognizer);
          };
          recognizer.postMessage('');
      };

      // This updates the UI when the app might get ready
      // Only when both recorder and recognizer are ready do we enable the buttons
      function updateUI() {
        console.log("[ updateUI Called ]");
        if (recorderReady && recognizerReady) startBtn.disabled = stopBtn.disabled = false;
      };

      // Callback function once the user authorises access to the microphone
      // in it, we instanciate the recorder
      function startUserMedia(stream) {
        console.log("[ startUserMediaStream Called ]");
        var input = audioContext.createMediaStreamSource(stream);
        // Firefox hack https://support.mozilla.org/en-US/questions/984179
        window.firefox_audio_hack = input; 
        var audioRecorderConfig = {errorCallback: function(x) {console.log("** Error from recorder: " + x);}}; // Status change
        recorder = new AudioRecorder(input, audioRecorderConfig);
        // If a recognizer is ready, we pass it to the recorder
        if (recognizer) recorder.consumers = [recognizer];
        recorderReady = true;
        updateUI();
        console.log("** Audio recorder ready");    // Status change
      };

      // This starts recording. We first need to get the id of the grammar to use
      var startRecording = function() {
        if (recorder && recorder.start()) console.log("isRecording?: true"); // Show the listening indicator.
      };

      // Stops recording
      var stopRecording = function() {
        recorder && recorder.stop();
        console.log("isRecording?: false");
      };

      // Called once the recognizer is ready
      // We then add the grammars to the input select tag and update the UI
      var recognizerReady = function() {
           console.log("[ recognizerReady Called ]");
           console.log(recognizer);
           recognizerReady = true;
           updateUI();
           console.log("** Recognizer ready");  // Status Change
      };

      // This adds a grammar from the grammars array
      // We add them one by one and call it again as
      // a callback.
      // Once we are done adding all grammars, we can call
      // recognizerReady()
      var feedGrammar = function(grammar) {
           console.log("[ feedGrammar Called ]");
      	  postRecognizerJob({command: 'addGrammar', data: grammar}, function() {recognizerReady();});
      };

      // This adds words to the recognizer. When it calls back, we add grammars
      var feedWords = function(words) {
           console.log("[ feedWords Called ]");
           postRecognizerJob({command: 'addWords', data: words}, function() {feedGrammar(grammar, 0);});
      };

      // This initializes the recognizer. When it calls back, we add words
      var initRecognizer = function() {
          console.log("[ initRecognizer Called ]");
          // You can pass parameters to the recognizer, such as : {command: 'initialize', data: [["-hmm", "my_model"], ["-fwdflat", "no"]]}
          postRecognizerJob({command: 'initialize'}, function() {
               if (recorder) {
                  recorder.consumers = [recognizer];
               }
               feedWords(wordList);
          });
      };

      // When the page is loaded, we spawn a new recognizer worker and call getUserMedia to
      // request access to the microphone
      window.onload = function() {
        console.log("** [ window.onload triggered ] Initializing web audio and speech recognizer, waiting for approval to access the microphone");
        callbackManager = new CallbackManager();
        spawnWorker("js/recognizer.js", function(worker) {
            // This is the onmessage function, once the worker is fully loaded
            worker.onmessage = function(e) {
                // This is the case when we have a callback id to be called
                console.log('worker received a message');
                console.log(e);
                if (e.data.hasOwnProperty('id')) {
                  var clb = callbackManager.get(e.data['id']);
                  var data = {};
                  if ( e.data.hasOwnProperty('data')) {  // If there are parameters of the callback
                     data = e.data.data;                 // Add them
                  }
                  
                  if(clb) clb(data);                     // Then run the callback.
                }
                // This is a case when the recognizer has a new hypothesis
                if (e.data.hasOwnProperty('hyp')) {
                  var newHyp = e.data.hyp;
                  if (e.data.hasOwnProperty('final') &&  e.data.final) newHyp = "Final: " + newHyp;
                  console.log(newHyp);    // spit out the hypothesis
                }
                // This is the case when we have an error
                if (e.data.hasOwnProperty('status') && (e.data.status == "error")) {
                  console.log("** Error in " + e.data.command + " with code " + JSON.stringify(e.data.code));
                }
            };
            // Once the worker is fully loaded, we can call the initialize function
            initRecognizer();
        });

        // The following is to initialize Web Audio
        try {
          window.AudioContext = window.AudioContext || window.webkitAudioContext;
          navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
          window.URL = window.URL || window.webkitURL;
          audioContext = new AudioContext();
        } catch (e) {
          console.log("** Error initializing Web Audio browser");
        }
        
        if (navigator.getUserMedia) {
            navigator.getUserMedia({audio: true}, startUserMedia, function(e) {
               console.log("** No live audio input in this browser");
            });
        } else {
            console.log("** No web audio support in this browser");
        }

      // Wiring JavaScript to the UI
      var startBtn = document.getElementById('startBtn');
      var stopBtn = document.getElementById('stopBtn');
      startBtn.disabled = true;
      stopBtn.disabled = true;
      startBtn.onclick = startRecording;
      stopBtn.onclick = stopRecording;
      };

       // This is the list of words that need to be added to the recognizer
       // This follows the CMU dictionary format
      var wordList = [["Deploy", "D IY P L OY"], ["Deploy(2)", "D AH P L OY"], ["the", "TH AH"], ["latest", "L AA T EH S T"], ["tag", "T AE G"], ["of", "AH V"], ["to", "T UW"],
           ["IntelliQueue", "IH N T EH L AH K Y UW"], ["IntelliQueue(2)", "IH N T EH L IY K Y UW"], ["IntelliStats", "IH N T EH L AH S T AE T S"], ["IntelliStats(2)", "IH N T EH L IY S T AE T S"], 
           ["IntelliDial", "IH N T EH L EH D AY AE L"], ["IntelliDial(2)", "IH N T EH L IY D AY AE L"], 
           ["QTS", "K Y UW T IY EH S"], ["L3D", "EH L TH R IY D IY"], ["CFP", "S IY EH F P IY"], ["DR", "D IY AA R"]];

      // This is to play with beloved or belated OSes
      var grammar = {numStates: 9, start: 0, end: 8, transitions: [{from: 0, to: 1, word: "Deploy"}, {from: 1, to: 2, word: "the"}, {from: 2, to: 3, word: "latest"}, {from: 3, to: 4, word: "tag"}, {from: 4, to: 5, word: "of"},
         {from: 5, to: 6, word: "IntelliQueue"}, {from: 5, to: 6, word: "IntelliStats"}, {from: 5, to: 6, word: "IntelliDial"}, {from: 6, to: 7, word: "to"}, {from: 7, to: 8, word: "QTS"}, {from: 7, to: 8, word: "L3D"}, {from: 7, to: 8, word: "CFP"}, {from: 7, to: 8, word: "DR"}]};
    </script>
    <!-- These are the two JavaScript files you must load in the HTML,
    The recognizer is loaded through a Web Worker -->
    <script src="js/audioRecorder.js"></script>
    <script src="js/callbackManager.js"></script>
  </body>
</html>
